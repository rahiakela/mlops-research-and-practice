{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic-data-pipeline-using-apache-beam.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM4xGKmx3Oc/8RfGdmHetx/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/building-machine-learning-pipelines/blob/main/02-introduction-to-tensorflow-extended/basic_data_pipeline_using_apache_beam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0lO1ss8iYs2"
      },
      "source": [
        "## Basic Data Pipeline using Apache Beam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGcINLbQiwbk"
      },
      "source": [
        "A variety of TFX components and libraries (e.g., TensorFlow Transform) rely on\n",
        "Apache Beam to process pipeline data efficiently. \n",
        "\n",
        "Apache Beam can be used to describe batch processes, streaming operations, and data pipelines. In fact, TFX relies on Apache Beam and uses it under\n",
        "the hood for a variety of components (e.g., TensorFlow Transform or TensorFlow\n",
        "Data Validation).\n",
        "\n",
        "While Apache Beam abstracts away the data processing logic from its supporting\n",
        "runtime tools, it can be executed on multiple distributed processing runtime environments.\n",
        "This means that you can run the same data pipeline on Apache Spark or Google\n",
        "Cloud Dataflow without a single change in the pipeline description. Also, Apache\n",
        "Beam was not just developed to describe batch processes but to support streaming\n",
        "operations seamlessly.\n",
        "\n",
        "Apache Beam’s abstraction is based on two concepts: collections and transformations.\n",
        "\n",
        "On the one hand, Beam’s collections describe operations where data is being\n",
        "read or written from or to a given file or stream.\n",
        "\n",
        "On the other hand, Beam’s transformations describe ways to manipulate the data.\n",
        "\n",
        "When we define our collections or transformations in our following\n",
        "example, no data is actually being loaded or transformed. This only happens\n",
        "when the pipeline is executed in the context of a runtime environment (e.g., Apache Beam’s DirectRunner, Apache Spark, Apache Flink, or Google Cloud Dataflow).\n",
        "\n",
        "Data pipelines usually start and end with data being read or written, which is handled in Apache Beam through collections, often called PCollections. The collections are then transformed, and the final result can be expressed as a collection again and written to a filesystem.\n",
        "\n",
        "The following example shows how to read a text file and return all lines:\n",
        "\n",
        "```python\n",
        "import apache_beam as beam\n",
        "\n",
        "with beam.Pipeline() as p:\n",
        "  lines = p | beam.io.ReadFromText(input_file)\n",
        "```\n",
        "\n",
        "Similar to the ReadFromText operation, Apache Beam provides functions to write collections to a text file (e.g., WriteToText). The write operation is usually performed after all transformations have been executed:\n",
        "\n",
        "```python\n",
        "with beam.Pipeline() as p:\n",
        "  ...\n",
        "  output | beam.io.WriteToText(output_file)\n",
        "```\n",
        "\n",
        "\n",
        "In Apache Beam, data is manipulated through transformations.The transformations can be chained by using the pipe operator |. If you chain multiple transformations of the same type, you have to provide a name for the operation, noted by the string identifier between the pipe operator and the right-angle brackets.\n",
        "\n",
        "In the following example, we apply all transformations sequentially on our lines extracted from the text file:\n",
        "\n",
        "```python\n",
        "counts = (\n",
        "  lines\n",
        "  | 'Split' >> beam.FlatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))\n",
        "  | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
        "  | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
        "```\n",
        "\n",
        "Let’s walk through this code in detail. As an example, we’ll take the phrases “Hello, how do you do?” and “I am well, thank you.”\n",
        "\n",
        "The Split transformation uses re.findall to split each line into a list of tokens, giving the result:\n",
        "\n",
        "```\n",
        "[\"Hello\", \"how\", \"do\", \"you\", \"do\"]\n",
        "[\"I\", \"am\", \"well\", \"thank\", \"you\"]\n",
        "```\n",
        "\n",
        "beam.FlatMap maps the result into a PCollection:\n",
        "\n",
        "```\n",
        "\"Hello\" \"how\" \"do\" \"you\" \"do\" \"I\" \"am\" \"well\" \"thank\" \"you\"\n",
        "```\n",
        "\n",
        "Next, the PairWithOne transformation uses beam.Map to create a tuple out of every token and the count (1 for each result):\n",
        "\n",
        "```\n",
        "(\"Hello\", 1) (\"how\", 1) (\"do\", 1) (\"you\", 1) (\"do\", 1) (\"I\", 1) (\"am\", 1)\n",
        "(\"well\", 1) (\"thank\", 1) (\"you\", 1)\n",
        "```\n",
        "\n",
        "Finally, the GroupAndSum transformation sums up all individual tuples for each token:\n",
        "\n",
        "```\n",
        "(\"Hello\", 1) (\"how\", 1) (\"do\", 2) (\"you\", 2) (\"I\", 1) (\"am\", 1) (\"well\", 1)\n",
        "(\"thank\", 1)\n",
        "```\n",
        "\n",
        "Apache Beam provides a variety of predefined transformations. However, if your preferred operation isn’t available, you can write your own transformations by using the Map operators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgQtIBmfk_l3"
      },
      "source": [
        "## Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhqYPmnpieJc"
      },
      "source": [
        "!pip install apache-beam[gcp]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8RzZg8tej2x",
        "outputId": "eb0e7106-ae88-4c93-90e9-bf8cfb78bff2"
      },
      "source": [
        "%%writefile basic_pipeline.py\n",
        "import re\n",
        "import apache_beam as beam\n",
        "from apache_beam.io import ReadFromText\n",
        "from apache_beam.io import WriteToText\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam.options.pipeline_options import SetupOptions\n",
        "\n",
        "input_file = \"gs://dataflow-samples/shakespeare/kinglear.txt\"\n",
        "output_file = \"output.txt\"\n",
        "\n",
        "# Define pipeline options object.\n",
        "pipeline_options = PipelineOptions()\n",
        "\n",
        "# Set up the Apache Beam pipeline.\n",
        "with beam.Pipeline(options=pipeline_options) as p:\n",
        "  # Read the text file or file pattern into a PCollection.\n",
        "  lines = p | ReadFromText(input_file)\n",
        "\n",
        "  # Perform the transformations on the collection: Count the occurrences of each word.\n",
        "  counts = (\n",
        "      lines | \"Split\" >> beam.FlatMap(lambda x: re.findall(r\"[A-Za-z\\']+\", x))\n",
        "            | \"PairWithOne\" >> beam.Map(lambda x: (x, 1))\n",
        "            | \"GroupAndSum\" >> beam.CombinePerKey(sum)\n",
        "  )\n",
        "\n",
        "  # Format the counts into a PCollection of strings.\n",
        "  def format_result(word_count):\n",
        "    (word, count) = word_count\n",
        "    return \"{}: {}\".format(word, count)\n",
        "\n",
        "  output = counts | \"Format\" >> beam.Map(format_result)\n",
        "\n",
        "  # Write the output using a \"Write\" transform that has side effects.\n",
        "  output | WriteToText(output_file)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing basic_pipeline.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGFwQy88gnqY"
      },
      "source": [
        "## Executing Your Basic Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVJa3jEiDBH"
      },
      "source": [
        "As an example, you can run the pipeline with Apache Beam’s DirectRunner by executing the following command (assuming that the previous example code was saved as `basic_pipeline.py`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br5CsvAAgJfY"
      },
      "source": [
        "!python basic_pipeline.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCWBgCD7iJy1"
      },
      "source": [
        "The results of the transformations can be found in the designated text file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axDPKWyxhGMn",
        "outputId": "97c7c02f-f448-4b45-85d4-9dea3589463c"
      },
      "source": [
        "!head output.txt*"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KING: 243\n",
            "LEAR: 236\n",
            "DRAMATIS: 1\n",
            "PERSONAE: 1\n",
            "king: 65\n",
            "of: 447\n",
            "Britain: 2\n",
            "OF: 15\n",
            "FRANCE: 10\n",
            "DUKE: 3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}