{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-algorithmic-dimensionality-reduction.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNv0BvIjpeYTWtjnFEllsH+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/mlops-research-and-practice/blob/main/MLOps-Specialization/course-3-machine-learning-modeling-pipelines-in-production/week-2-model-resource-management-techniques/02_algorithmic_dimensionality_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Algorithmic Dimensionality Reduction "
      ],
      "metadata": {
        "id": "G37uXh3Cv7WP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome, during this ungraded lab you are going to perform several algorithms that aim to reduce the dimensionality of data. This topic is very important because it is not uncommon that reduced models outperform the ones trained on the raw data because noise and redundant information are present in most datasets. This will also allow your models to train and make predictions faster, which might be really important depending on the problem you are working on. In particular you will:\n",
        "\n",
        "\n",
        "1. Use Principal Component Analysis (**PCA**) to reduce the dimensionality of a dataset that classifies celestial bodies.\n",
        "2. Use Single Value Decomposition (**SVD**) to create low level representations of images of handwritten digits.\n",
        "3. Use Non-negative Matrix Factorization (**NMF**) to segment text into topics.\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "ZGDAAxoev_Fh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "kUkgvZ3wwJ3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General use imports\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "O8PyQ_RlwMsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download zip file\n",
        "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip -o HTRU2.zip\n",
        "\n",
        "# Unzip it\n",
        "data_folder = os.path.join('.', 'data')\n",
        "import zipfile\n",
        "with zipfile.ZipFile('HTRU2.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(data_folder)\n",
        "    \n",
        "# Delete the downloaded zip file\n",
        "os.remove('HTRU2.zip')"
      ],
      "metadata": {
        "id": "G0m7UroXwy2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(data_folder)"
      ],
      "metadata": {
        "id": "YM-Dm8NPw0hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal Components Analysis - PCA"
      ],
      "metadata": {
        "id": "ajQ1Xc2vwrFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an unsupervised algorithm that creates linear combinations of the original features. PCA is a widely used technique for dimension reduction since it is fast and easy to implement. PCA aims to keep as much variance as possible from the original data in a lower dimensional space. It finds the best axis to project the data so that the variance of the projections is maximized.\n",
        "\n",
        "In the lecture you saw PCA applied to the Iris dataset. This dataset has been used extensively to showcase PCA so here you are going to do something different. You are going to use the [HTRU_2](https://archive.ics.uci.edu/ml/datasets/HTRU2) dataset which describes several celestial objects and the idea is to be able to classify if an object is a pulsar star or not.\n",
        "\n",
        "\n",
        "Load the data into a dataframe for easier inspection:"
      ],
      "metadata": {
        "id": "71y2mnfMwreX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5lmvxwwKw796"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}