{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-distributed-strategies-with-tf-and-keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMRrAGJ5vJrHRUbGdBBisX2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/mlops-research-and-practice/blob/main/MLOps-Specialization/course-3-machine-learning-modeling-pipelines-in-production/week-3-high-performance-modeling/01_distributed_strategies_with_tf_and_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distributed Strategies with TF and Keras"
      ],
      "metadata": {
        "id": "TSYQCj47P3C3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome, during this ungraded lab you are going to perform a distributed training strategy using TensorFlow and Keras, specifically the [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy). \n",
        "\n",
        "With the help of this strategy, a Keras model that was designed to run on single-worker can seamlessly work on multiple workers with minimal code change. In particular you will:\n",
        "\n",
        "\n",
        "1. Perform training with a single worker.\n",
        "2. Understand the requirements for a multi-worker setup (`tf_config` variable) and using context managers for implementing distributed strategies.\n",
        "3. Use magic commands to simulate different machines.\n",
        "4. Perform a multi-worker training strategy.\n",
        "\n",
        "This notebook is based on the official [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) notebook, which covers some additional topics in case you want a deeper dive into this topic.\n",
        "\n",
        "[Distributed Training with TensorFlow](https://www.tensorflow.org/guide/distributed_training) guide is also available for an overview of the distribution strategies TensorFlow supports for those interested in a deeper understanding of `tf.distribute.Strategy` APIs.\n",
        "\n",
        "Let's get started!"
      ],
      "metadata": {
        "id": "FO6owdJYRQ2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "qbUfahqzRRgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "VuMuKDbKRTj-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before importing TensorFlow, make a few changes to the environment.\n",
        "\n",
        "- Disable all GPUs. This prevents errors caused by the workers all trying to use the same GPU. **For a real application each worker would be on a different machine.**\n",
        "\n",
        "\n",
        "- Add the current directory to python's path so modules in this directory can be imported."
      ],
      "metadata": {
        "id": "wkMg0S6nRZuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable GPUs\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
        "\n",
        "# Add current directory to path\n",
        "if '.' not in sys.path:\n",
        "  sys.path.insert(0, '.')"
      ],
      "metadata": {
        "id": "at7qos5qRaGd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous step is important since this notebook relies on writting files using the magic command `%%writefile` and then importing them as modules.\n",
        "\n",
        "Now that the environment configuration is ready, import TensorFlow.\n"
      ],
      "metadata": {
        "id": "b5SzPkb3RcEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('TensorFlow Version:', tf.__version__)\n",
        "\n",
        "# Ignore warnings\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWcTTbvwRd6q",
        "outputId": "4a2129d5-6aeb-43c3-c752-7e68bc60c4d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version: 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and model definition"
      ],
      "metadata": {
        "id": "Xy6dYYaxRf9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next create an `mnist.py` file with a simple model and dataset setup. This python file will be used by the worker-processes in this tutorial.\n",
        "\n",
        "The name of this file derives from the dataset you will be using which is called [mnist](https://keras.io/api/datasets/mnist/) and consists of 60,000 28x28 grayscale images of the first 10 digits."
      ],
      "metadata": {
        "id": "sB7q8oS3RjC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mnist.py\n",
        "\n",
        "# import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def mnist_dataset(batch_size):\n",
        "  # load the data\n",
        "  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "  # Normalize pixel values for x_train and cast to float32\n",
        "  x_train = x_train / np.float32(255)\n",
        "  # Cast y_train to int64\n",
        "  y_train = y_train.astype(np.int64)\n",
        "  # Define repeated and shuffled dataset\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n",
        "  return train_dataset\n",
        "\n",
        "def build_and_compile_cnn_model():\n",
        "  # Define simple CNN model using Keras Sequential\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(10)                         \n",
        "  ])\n",
        "\n",
        "  # compile the model\n",
        "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pefWD8HZRk9W",
        "outputId": "f7569c84-42b5-4599-8ae0-1ed194a494ad"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mnist.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the file was succesfully created:"
      ],
      "metadata": {
        "id": "U9CgfQc_ZFeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls *.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNjla-RPZF3E",
        "outputId": "3564298a-3f5a-434d-e535-fb4b798c9238"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mnist.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the mnist module you just created and try training the model for a small number of epochs to observe the results of a single worker to make sure everything works correctly."
      ],
      "metadata": {
        "id": "VO7zd0JvZKoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your mnist model\n",
        "import mnist\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Load the dataset\n",
        "single_worker_dataset = mnist.mnist_dataset(batch_size)\n",
        "\n",
        "# Load compiled CNN model\n",
        "single_worker_model = mnist.build_and_compile_cnn_model()\n",
        "\n",
        "# As training progresses, the loss should drop and the accuracy should increase.\n",
        "single_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI7F6nvOZLPT",
        "outputId": "9072be38-1295-46e8-8e69-4485020810f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "70/70 [==============================] - 3s 40ms/step - loss: 2.2648 - accuracy: 0.2797\n",
            "Epoch 2/3\n",
            "70/70 [==============================] - 3s 40ms/step - loss: 2.2021 - accuracy: 0.4808\n",
            "Epoch 3/3\n",
            "70/70 [==============================] - 3s 40ms/step - loss: 2.1261 - accuracy: 0.6132\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f04862d74d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Everything is working as expected! \n",
        "\n",
        "Now you will see how multiple workers can be used as a distributed strategy."
      ],
      "metadata": {
        "id": "yCDdoyPvadcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-worker Configuration"
      ],
      "metadata": {
        "id": "Bf8tjWzXad4k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's enter the world of multi-worker training. In TensorFlow, the `TF_CONFIG` environment variable is required for training on multiple machines, each of which possibly has a different role. `TF_CONFIG` is a JSON string used to specify the cluster configuration on each worker that is part of the cluster.\n",
        "\n",
        "There are two components of `TF_CONFIG`: `cluster` and `task`. \n",
        "\n",
        "Let's dive into how they are used:\n",
        "\n",
        "`cluster`:\n",
        "- **It is the same for all workers** and provides information about the training cluster, which is a dict consisting of different types of jobs such as `worker`.\n",
        "\n",
        "- In multi-worker training with `MultiWorkerMirroredStrategy`, there is usually one `worker` that takes on a little more responsibility like saving checkpoint and writing summary file for TensorBoard in addition to what a regular `worker` does. \n",
        "-Such a worker is referred to as the `chief` worker, and it is customary that the `worker` with `index` 0 is appointed as the chief `worker` (in fact this is how `tf.distribute.Strategy` is implemented).\n",
        "\n",
        "`task`:\n",
        "- Provides information of the current task and is different on each worker. It specifies the `type` and `index` of that worker. \n",
        "\n",
        "Here is an example configuration:"
      ],
      "metadata": {
        "id": "9DYbwthnaiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kYQ4unU6akmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}